{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from os import listdir\nfrom numpy import array\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom pickle import dump\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Flatten\nimport string\nfrom pickle import load\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers.merge import add\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import plot_model\nfrom numpy import argmax\nfrom keras.models import load_model\nfrom nltk.translate.bleu_score import corpus_bleu \nfrom PIL import Image \nimport matplotlib.pyplot as plt\nimport numpy as np\n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare photo data\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract features from each photo in directory\ndef extract_features(directory):\n    model=VGG16()\n    model.layers.pop()\n    model=Model(inputs=model.inputs,outputs=model.layers[-1].output)\n    features=dict()\n    for name in listdir(directory):\n        filename=directory+'/'+name\n        image=load_img(filename,target_size=(224,224))\n        image=img_to_array(image)\n        image=image.reshape(1,image.shape[0],image.shape[1],image.shape[2])\n        image=preprocess_input(image)\n        feature=model.predict(image,verbose=0)\n        image_id=name.split('.')[0]\n        features[image_id]=feature\n    return features\ndirc='../input/flickr8k-sau/Flickr_Data/Images/'\nfeatures=extract_features(dirc)\ndump(features,open('features.pkl','wb'))\nprint(len(features))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load photo features\ndef load_photo_features(filename,dataset):\n    all_features=load(open(filename,'rb'))        #load all features\n    features={k: all_features[k] for k in dataset} #filter features\n    return features\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare text data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#load doc into memory\ndef load_doc(filename):\n    file= open(filename,'r') #read only\n    text=file.read()\n    file.close()\n    return text\n \n#extract descriptions for images\ndef load_descriptions(filename):\n    doc=load_doc(filename)\n    mapping=dict()\n    for line in doc.split('\\n'):\n        tokens=line.split()\n        if len(line)<2:\n            continue\n        image_id,image_desc=tokens[0],tokens[1:]\n        image_id=image_id.split('.')[0]\n        image_desc=' '.join(image_desc)\n        if image_id not in  mapping:\n            mapping[image_id]=list()\n        mapping[image_id].append(image_desc)\n    return mapping\n\ndef clean_descriptions(descriptions):\n    table=str.maketrans('','',string.punctuation) #prepare translation table for removing punctuation\n    for key,desc_list in descriptions.items():\n        for i in range(len(desc_list)):\n            desc=desc_list[i]      \n            desc=desc.split()                     #tokenize  \n            desc=[word.lower() for word in desc]  #convert to lower case\n            desc=[w.translate(table) for w in desc]   #remove punctuations\n            desc=[word for word in desc if len(word)>1] #remove hanging s and a\n            desc=[word for word in desc if word.isalpha()] #remove tokens with numbers in them\n            desc_list[i]=' '.join(desc) #store as string\n            \n#convert loaded desc into vocab of words\ndef to_vocabulary(descriptions):\n    all_desc=set()\n    for key in descriptions.keys():\n        [all_desc.update(d.split()) for d in descriptions[key]]\n    return all_desc\n\n#save descriptions to file, one per line\ndef save_descriptions(descriptions,filename):\n    lines=list()\n    for key, desc_list in descriptions.items():\n        for desc in desc_list:\n            lines.append(key+ ' '+ desc)\n    data='\\n'.join(lines)\n    file=open(filename,'w')\n    file.write(data)\n    file.close()\n\nfilen='../input/flickr8k-sau/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'\ndescriptions=load_descriptions(filen) #parse descriptions\nprint(len(descriptions))\nclean_descriptions(descriptions)\nvocabulary = to_vocabulary(descriptions)\nprint(len(vocabulary))\nsave_descriptions(descriptions, 'descriptions1.txt')\n\n    \n            \n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep learning Model\n### 1.Loading data\n### 2.Defining the model\n### 3.Fitting the model\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#load a predefined list of photo identifiers\ndef load_set(filename):\n    doc=load_doc(filename)\n    dataset=list()\n    for line in doc.split('\\n'):\n        if len(line) < 1:\n            continue\n        identifier = line.split('.')[0]\n        dataset.append(identifier)\n    return set(dataset)\n\n#load clean description into memory\ndef load_clean_descriptions(filename,dataset):\n    doc=load_doc(filename)\n    descriptions=dict()\n    for line in doc.split('\\n'):\n        tokens=line.split() #split line by white space\n        image_id,image_desc=tokens[0],tokens[1:] #split id with descrp\n        if image_id in dataset:\n            if image_id not in descriptions:    #create list of images in the set\n                    descriptions[image_id]=list()\n            desc='startseq '+ ' '.join(image_desc)+' endseq'    #wrap descr in tokens\n            descriptions[image_id].append(desc)\n    return descriptions\n        \n#convert a dictionary of clean_descriptions to a list of descriptions\ndef to_lines(descriptions):\n    all_desc=list()\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n    lines=to_lines(descriptions)\n    tokenizer=Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n#length of description with most words\ndef max_len(descriptions):\n    lines=to_lines(descriptions)\n    return max(len(d.split()) for d in lines)\n\n#create sequences of images, input_sequences and output words for an image\ndef create_sequences(tokenizer,max_length,desc_list,photo,vocab_size):\n    X1,X2,y=list(),list(),list()\n    for desc in desc_list: #iterating over each description for image\n        seq=tokenizer.texts_to_sequences([desc])[0] #encode the sequence\n        for i in range(1,len(seq)):  \n            in_seq,out_seq=seq[:i],seq[i]  # split into input and output pair\n            in_seq=pad_sequences([in_seq],maxlen=max_length)[0] # pad input sequence\n            out_seq=to_categorical([out_seq],num_classes=vocab_size)[0] #encode output sequences\n            #store\n            X1.append(photo)\n            X2.append(in_seq)\n            y.append(out_seq)\n    return array(X1),array(X2),array(y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define the captioning model\ndef define_model(vocab_size,max_lenth):\n    #feature extractor model\n    inputs1=Input(shape=(4096,))\n    fe1=Dropout(0.5)(inputs1)\n    fe2=Dense(256,activation='relu')(fe1)\n    #sequence model\n    inputs2=Input(shape=(max_length,))\n    se1=Embedding(vocab_size,256,mask_zero=True)(inputs2)\n    se2=Dropout(0.5)(se1)\n    se3=LSTM(256)(se2)\n    #decoder model\n    decoder1=add([fe2,se3])\n    decoder2=Dense(256,activation='relu')(decoder1)\n    outputs=Dense(vocab_size,activation='softmax')(decoder2)\n    #tie it together [image,seq][word]\n    model=Model(inputs=[inputs1,inputs2],outputs=outputs)\n    model.compile(loss='categorical_crossentropy',optimizer='adam')\n    return model\n\n#data generator, used in a call to model.fit_generator()\ndef data_generator(descriptions,photos,tokenizer,max_length,vocab_size):\n    while 1: #lopp for ever over images\n        for key,desc_list in descriptions.items():\n            photo=photos[key][0]     #retrieve the photo feature\n            in_img,in_seq,out_word=create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n            yield [[in_img,in_seq],out_word]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train dataset\n \n# load training dataset (6K)\nfilename = '../input/flickr8k-sau/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# photo features\ntrain_features = load_photo_features('features.pkl', train)\nprint('Photos: train=%d' % len(train_features))\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n# determine the maximum sequence length\nmax_length = max_len(train_descriptions)\nprint('Description Length: %d' % max_length)\n\n#test the data generator\ngenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\ninputs, outputs = next(generator)\nprint(inputs[0].shape)\nprint(inputs[1].shape)\nprint(outputs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#define the model\nmodel=define_model(vocab_size,max_length)\n#train the model and run epochs manually & save after each epoch\nepochs=20\nsteps=len(train_descriptions)\nfor i in range(epochs):\n    generator=data_generator(train_descriptions,train_features,tokenizer,max_length,vocab_size)\n    model.fit_generator(generator,epochs=1,steps_per_epoch=steps,verbose=1) #fir for 1 epoch\n    model.save('model_'+str(i)+'.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#map an integer to a word\ndef word_for_id(integer,tokenizer):\n    for word,index in tokenizer.word_index.items():\n        if index==integer:\n            return word\n    return None\n\n#generate a description for an image\ndef generate_desc(model,tokenizer,photo,max_length):\n    in_text='startseq' #seed the generation process\n    for i in range(max_length):\n        sequence=tokenizer.texts_to_sequences([in_text])[0]   #integer encode input sequence\n        sequence=pad_sequences([sequence],maxlen=max_length)  #pad input\n        yhat=model.predict([photo,sequence],verbose=0)   #predict next word\n        yhat=argmax(yhat)     #convert prob into integer\n        word=word_for_id(yhat,tokenizer)\n        if word is None:\n            break\n        in_text+=' '+word #append as input for generating the next word\n        if word=='endseq':\n            break\n    return in_text\n\n#evaluate the skill of the model\ndef evaluate_model(model,descriptions,photos,tokenizer,max_length):\n    actual,predicted= list(),list()\n    for key,desc_list in descriptions.items():\n        yhat=generate_desc(model,tokenizer,photos[key],max_length) #generate descrip\n        #store actual and predicted\n        references=[d.split() for d in desc_list]  \n        actual.append(references)\n        predicted.append(yhat.split())\n    #calculate BLUE score\n    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dev dataset\n \n# load test set\nfilename = '../input/flickr8k-sau/Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt'\ntest = load_set(filename)\nprint('Dataset: %d' % len(test))\n# descriptions\ntest_descriptions = load_clean_descriptions('descriptions.txt', test)\nprint('Descriptions: test=%d' % len(test_descriptions))\n# photo features\ntest_features = load_photo_features('features.pkl', test)\nprint('Photos: test=%d' % len(test_features))\n\n#load and evaluate\ni=19\nmod=load_model('model_'+str(i)+'.h5')\nevaluate_model(mod,test_descriptions,test_features,tokenizer,max_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate new caption","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#load doc into memory  load_doc()\n#load a pre-defined list of photo identifiers   load_set()\n#load clean descriptions into memory   load_clean_descriptions()\n#convert a dictionary to a list of descriptions   to_lines()\n#fit a tokenizer    create_tokenizer()\n#load training dataset\n#prepare tokenizer tokenizer=create_tokenizer(train_descriptions)\ndump(tokenizer,open('tokenizer.pkl','wb')) #saving the tokenizer\n \n#extract features from each photo in directory extract_features()\ndef ext_features(filename):\n    model=VGG16()\n    model.layers.pop()\n    model=Model(inputs=model.inputs,outputs=model.layers[-1].output)\n    image=load_img(filename,target_size=(224,224))\n    image=img_to_array(image)\n    image=image.reshape(1,image.shape[0],image.shape[1],image.shape[2])\n    image=preprocess_input(image)\n    feature=model.predict(image,verbose=0)\n    return feature\n#map integer to a word   word_for_id()\n#generate a description for an image generate_desc()\n\ntokenizer=load(open('tokenizer.pkl','rb')) #load the tokenizer\nmax_length= 34 #predefine the max sequence length\nmod=load_model('model_'+str(i)+'.h5')#load the model load_model()\nphoto = ext_features('../input/flickr8k-sau/Flickr_Data/Images/1020651753_06077ec457.jpg') #load and prepare the photograph\nimg_array = np.array(Image.open('../input/flickr8k-sau/Flickr_Data/Images/1020651753_06077ec457.jpg'))\nplt.imshow(img_array)\n\ndescription = generate_desc(mod, tokenizer, photo, max_length)#generate description\nprint(description)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
